# 20210520

BERT is trained using two pre-trained tasks.

1. masked token prediction
2. next sentence prediction

It  means we can exploit these mechanisms to predict on similar tasks such as fill in the blank tasks. 

